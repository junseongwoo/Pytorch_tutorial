{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "14612188b5106734f13b419466ff7a8ffd40ac17353063e9377077bed0ab83e2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 텐서(Tensor)\n",
    "### 텐서 : 배열(array)와 행렬(matrix)과 유사한 자료구조\n",
    "### Pytorch에서는 텐서를 이용하요 모델의 입력과 출력, 매개변수의 부호화를 한다.\n",
    "### 텐서는 GPU나 다른 하드웨어 가속기에서 실행하는 점만 제외하면 numpy의 ndarray와 매우 유사하다. \n",
    "### - 자동미분에 최적화가 되어있다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]]),\n",
       " torch.Tensor)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# 텐서 초기화 \n",
    "# 1. 데이터로부터 직접 생성하기\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data, type(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# 자료형은 자동으로 유추합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]], dtype=torch.int32),\n",
       " torch.Tensor)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# 2. Numpy 배열로부터 생성\n",
    "np_array = np.array(data)\n",
    "x_np = torch.tensor(np_array)\n",
    "x_np, type(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "type(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0.7201, 0.8759, 0.7510],\n",
       "         [0.5333, 0.2718, 0.5705]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# 3. 무작위(random)와 상수(constant) 값을 사용\n",
    "# shape는 텐서의 차원을 나타내는 튜플\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "rand_tensor, ones_tensor, zeros_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1333, 0.2686, 0.8532, 0.7953],\n",
       "        [0.9186, 0.9008, 0.2792, 0.5218],\n",
       "        [0.7256, 0.2703, 0.4026, 0.6121]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "### 텐서의 속성 \n",
    "# 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지 나타냄\n",
    "tensor = torch.rand(3,4) \n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.float32, device(type='cpu'))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tensor.shape, tensor.dtype, tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 텐서 연산\n",
    "# 기본적으로 텐서는 CPU에 생성된다 -> .to 메서드를 사용하면 GPU 가용성을 확인한 뒤 GPU로 이동할 수 있다. \n",
    "# 장치들 간에 큰 텐서들을 복사하는 것은 메모리와 시간이 많이 든다는 것을 기억 해야한다.\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Frist row :  tensor([1., 1., 1., 1.])\nFirst column :  tensor([1., 1., 1., 1.])\nLast Colunm :  tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# numpy 라이브러리에 익숙 하다면 tensor를 사용하기는 아주 쉽습니다. \n",
    "tensor = torch.ones(4,4)\n",
    "print(\"Frist row : \", tensor[0])\n",
    "print(\"First column : \", tensor[:, 0])\n",
    "print(\"Last Colunm : \", tensor[..., -1])\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 텐서 합치기 : torch.cat \n",
    "# 주어진 차원에 따라 텐서를 연결이 가능 \n",
    "tensor = torch.cat([tensor, tensor, tensor], dim=1) # 가로 합치기\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "tensor = torch.ones(4,4) \n",
    "tensor = torch.stack([tensor, tensor, tensor], dim=1) # 세로 합치기 \n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "### 산술 연산 \n",
    "# 두 텐서간의 행렬 곱 , y1, y2, y3는 다 같은 값이다.\n",
    "tensor = torch.ones(4,4) \n",
    "y1 = tensor @ tensor.T\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "y2 = tensor.matmul(tensor.T) # .T 는 역행렬 \n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.6230, 0.6839, 0.6466, 0.2027],\n",
       "        [0.3350, 0.8512, 0.9378, 0.6615],\n",
       "        [0.9681, 0.2372, 0.5259, 0.7314],\n",
       "        [0.7410, 0.9694, 0.4121, 0.6484]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "y3 = torch.rand_like(tensor)\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# 요소별 곱 계산 \n",
    "z1 = tensor*tensor\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "z2 = tensor.mul(tensor)\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2030, 0.6912, 0.8390, 0.4405],\n",
       "        [0.7670, 0.7544, 0.0211, 0.0521],\n",
       "        [0.1104, 0.9401, 0.5529, 0.1126],\n",
       "        [0.5895, 0.9016, 0.7845, 0.8932]])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "z3 = torch.rand_like(tensor)\n",
    "z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(16.)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# 단일요소 텐서의 모든 값을 하나로 집계하여 요소가 하나인 텐서인 경우 item()을 사용하여 숫자로 변환 가능\n",
    "agg = tensor.sum()\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(16.0, float)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "agg.item(), type(agg.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# 바꿔치기 연산 : 연산결과를 피연산자에 저장하는 연산 \n",
    "# _ 접미사를 붙인다\n",
    "tensor = torch.ones(4,4) \n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6.],\n",
       "        [6., 6., 6., 6.]])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "tensor.add_(5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : tensor([1., 1., 1., 1., 1.])\nn : [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Numpy 변환\n",
    "# cpu상 텐서와 numpy 배열은 메모리 공간 공유 -> 하나를 변경하면 다른 하나도 변경\n",
    "# 텐서 -> numpy 배열 변경 \n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "print(f\"t : {t}\")\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : tensor([2., 2., 2., 2., 2.])\nn : [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t : {t}\")\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn : [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# numpy -> tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t : {t}\")\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}